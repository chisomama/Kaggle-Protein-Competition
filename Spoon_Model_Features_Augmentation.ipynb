{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "from imageio import imread\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from multiprocessing import Pool\n",
    "from skimage.filters import sobel_h,sobel_v\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input, load_model\n",
    "from keras.layers import BatchNormalization, add, Dropout, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.utils import Sequence\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import cv2\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "from imageio import imread\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from multiprocessing import Pool\n",
    "from skimage.filters import sobel_h,sobel_v\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input, load_model\n",
    "from keras.layers import BatchNormalization, add,Cropping2D, Dropout, Conv2D, MaxPooling2D, Dense, Flatten, Dropout, concatenate, UpSampling2D\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = \"/Users/mohamed/Desktop/Kaggle-Protein/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(\"/Users/mohamed/Desktop/Kaggle-Protein/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitter = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partitions = []\n",
    "\n",
    "for train_idx, test_idx in splitter.split(train_labels.index.values):\n",
    "    partition = {}\n",
    "    partition[\"train\"] = train_labels.Id.values[train_idx]\n",
    "    partition[\"validation\"] = train_labels.Id.values[test_idx]\n",
    "    partitions.append(partition)\n",
    "    #print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    #print(\"TRAIN:\", len(train_idx), \"TEST:\", len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_path = \"/Users/mohamed/Desktop/Kaggle-Protein/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold for prediction: https://www.kaggle.com/iafoss/pretrained-resnet34-with-rgby-0-460-public-lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb_prob = [\n",
    " 0.362397820,0.043841336,0.075268817,0.059322034,0.075268817,\n",
    " 0.075268817,0.043841336,0.075268817,0.010000000,0.010000000,\n",
    " 0.010000000,0.043841336,0.043841336,0.014198783,0.043841336,\n",
    " 0.010000000,0.028806584,0.014198783,0.028806584,0.059322034,\n",
    " 0.010000000,0.126126126,0.028806584,0.075268817,0.010000000,\n",
    " 0.222493880,0.028806584,0.010000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(basepath, image_id):\n",
    "\n",
    "    images = np.zeros(shape=(512,512,4))\n",
    "    images[:,:,0] = Image.open(basepath + \"/\" + image_id + \"_green\" + \".png\")\n",
    "    images[:,:,1] = Image.open(basepath + \"/\" + image_id + \"_red\" + \".png\")\n",
    "    images[:,:,2] = Image.open(basepath + \"/\" + image_id + \"_blue\" + \".png\")\n",
    "    images[:,:,3] = Image.open(basepath + \"/\" + image_id + \"_yellow\" + \".png\")\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform labels in binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label(file_ids, train_labels):\n",
    "    l=train_labels[train_labels.Id.isin(file_ids)][\"Target\"].values\n",
    "    l= [[int(i) for i in s.split()] for s in l]\n",
    "    labels = []\n",
    "    for i in l:\n",
    "        L = [0]*28\n",
    "        for j in i:\n",
    "            L[j]=1\n",
    "        labels.append(L)\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4110074026392018\n",
      "0.04184100418410042\n",
      "0.115545542323785\n",
      "0.05133569359510782\n",
      "0.06002574831026714\n",
      "0.08255551979401352\n",
      "0.03427743804312842\n",
      "0.09349855165754747\n",
      "0.002574831026713872\n",
      "0.001126488574187319\n",
      "0.000643707756678468\n",
      "0.035082072738976504\n",
      "0.023495333118764082\n",
      "0.017219182491149017\n",
      "0.03057611844222723\n",
      "0.00048278081750885096\n",
      "0.0160926939169617\n",
      "0.006598004505954297\n",
      "0.03105889925973608\n",
      "0.051496620534277435\n",
      "0.007885420019311232\n",
      "0.123109108464757\n",
      "0.0246218216929514\n",
      "0.09221113614419053\n",
      "0.01013839716768587\n",
      "0.2597360798197618\n",
      "0.010621177985194722\n",
      "0.000160926939169617\n"
     ]
    }
   ],
   "source": [
    "for i in get_label(partition[\"validation\"], train_labels).mean(axis=0):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(image):\n",
    "    grad_x0=sobel_h(image[:,:,0])\n",
    "    grad_y0=sobel_v(image[:,:,0])\n",
    "    grad0=np.sqrt(grad_x0*grad_x0+grad_y0*grad_y0).T\n",
    "    \n",
    "    grad_x1=sobel_h(image[:,:,1])\n",
    "    grad_y1=sobel_v(image[:,:,1])\n",
    "    grad1=np.sqrt(grad_x1*grad_x1+grad_y1*grad_y1).T\n",
    "    \n",
    "    grad_x2=sobel_h(image[:,:,2])\n",
    "    grad_y2=sobel_v(image[:,:,2])\n",
    "    grad2=np.sqrt(grad_x2*grad_x2+grad_y2*grad_y2).T\n",
    "    \n",
    "    grad_x3=sobel_h(image[:,:,3])\n",
    "    grad_y3=sobel_v(image[:,:,3])\n",
    "    grad3=np.sqrt(grad_x3*grad_x3+grad_y3*grad_y3).T\n",
    "    \n",
    "    return np.array([grad0,grad1,grad2, grad3]).T\n",
    "\n",
    "def grad_threshold(image, eps):\n",
    "    \n",
    "    return (grad(image) > eps)*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dirrectional_grad(image,theta):\n",
    "    \n",
    "    grad_x0 = np.cos(theta)*sobel_h(image[:,:,0]) + np.sin(theta)*sobel_v(image[:,:,0])\n",
    "    grad_x1 = np.cos(theta)*sobel_h(image[:,:,1]) + np.sin(theta)*sobel_v(image[:,:,1])\n",
    "    grad_x2 = np.cos(theta)*sobel_h(image[:,:,2]) + np.sin(theta)*sobel_v(image[:,:,2])\n",
    "    grad_x3 = np.cos(theta)*sobel_h(image[:,:,3]) + np.sin(theta)*sobel_v(image[:,:,3])\n",
    "    \n",
    "    grad0= np.maximum(grad_x0,0).T\n",
    "    grad1= np.maximum(grad_x1,0).T\n",
    "    grad2= np.maximum(grad_x2,0).T\n",
    "    grad3= np.maximum(grad_x3,0).T\n",
    "    image= np.array([grad0,grad1,grad2, grad3]).T\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the shape according to preprocessing choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape(grad, dir_grad, grad_threshold, nb_rot, nb_threshold, block_size):\n",
    "    if block_size != 0: \n",
    "        h = 512//block_size\n",
    "        h_rest = (512%block_size)>0\n",
    "    else:\n",
    "        h, h_rest= 512, 0\n",
    "        \n",
    "    return (h+h_rest, h+h_rest, 4 + 4*grad + dir_grad*4*len(np.arange(0, 360, 360//nb_rot)) + grad_threshold*4*len(np.arange(0,128,128//nb_threshold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fbeta_score_macro(y_true, y_pred, beta=1, threshold=lb_prob):\n",
    "\n",
    "    y_true = K.cast(y_true, 'float')\n",
    "    y_pred = K.cast(K.greater(K.cast(y_pred, 'float'), threshold), 'float')\n",
    "\n",
    "    tp = K.sum(y_true * y_pred, axis=0)\n",
    "    fp = K.sum((1 - y_true) * y_pred, axis=0)\n",
    "    fn = K.sum(y_true * (1 - y_pred), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = (1 + beta ** 2) * p * r / ((beta ** 2) * p + r + K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, list_IDs, labels_path, shuffle, batch_size, basepath, grad, dir_grad, grad_threshold, nb_threshold, nb_rot, reduce, block_size):\n",
    "\n",
    "        self.labels = pd.read_csv(labels_path)\n",
    "        self.list_IDs = list_IDs\n",
    "        self.batch_size = batch_size\n",
    "        self.shape_features = shape(grad, dir_grad, grad_threshold, nb_rot, nb_threshold, block_size)\n",
    "        self.batch_shape = (self.batch_size, self.shape_features[0], self.shape_features[1], self.shape_features[2]) \n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.basepath = basepath\n",
    "        self.grad = grad\n",
    "        self.dir_grad = dir_grad\n",
    "        self.grad_threshold = grad_threshold\n",
    "        self.nb_threshold = nb_threshold \n",
    "        self.nb_rot = nb_rot\n",
    "        self.reduce = reduce\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def Reduce(self, images):\n",
    "        block=(self.block_size,self.block_size,1)\n",
    "        return block_reduce(images,block,np.mean)\n",
    "        \n",
    "    def features_aumentation(self, image):    \n",
    "\n",
    "        grad_=self.grad\n",
    "        dir_grad_=self.dir_grad\n",
    "        grad_threshold_=self.grad_threshold\n",
    "\n",
    "        a=image\n",
    "\n",
    "        if grad_:\n",
    "            a=np.append(a,grad(image), axis=2)\n",
    "\n",
    "        if dir_grad_:\n",
    "            rot=np.arange(0, 360, 360//self.nb_rot)\n",
    "            for i in rot:\n",
    "                a=np.append(a,dirrectional_grad(image,i), axis=2)\n",
    "\n",
    "        if grad_threshold_:\n",
    "            eps=np.arange(0,128,128//self.nb_threshold)\n",
    "            for e in eps:      \n",
    "                a=np.append(a, grad_threshold(image,e), axis=2)\n",
    "\n",
    "        return a\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def get_loader(self,list_id):\n",
    "\n",
    "        filenames = []\n",
    "        idx = 0\n",
    "        images = np.zeros(self.batch_shape)\n",
    "\n",
    "        for image_id in list_id:\n",
    "            if self.reduce:\n",
    "                images[idx,:,:,:] = self.Reduce(self.features_aumentation(load_image(self.basepath, image_id)))\n",
    "            else:\n",
    "                images[idx,:,:,:] = self.features_aumentation(load_image(self.basepath, image_id))\n",
    "            \n",
    "            filenames.append(image_id)\n",
    "            idx += 1\n",
    "            if idx == self.batch_shape[0]:\n",
    "                images = np.array(images)\n",
    "\n",
    "                return filenames, get_label(filenames,self.labels), images\n",
    "                \n",
    "                filenames = []\n",
    "                images = np.zeros(self.batch_shape)\n",
    "                idx = 0\n",
    "        \n",
    "        if idx > 0:\n",
    "\n",
    "            images = np.array(images)\n",
    "            return filenames, get_label(filenames,self.labels), images\n",
    "   \n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        Ids, y, X = self.get_loader(list_IDs_temp)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PredictGenerator:\n",
    "    \n",
    "    def __init__(self, predict_Ids, predict_path):\n",
    "        self.basepath = predict_path\n",
    "        self.identifiers = predict_Ids        \n",
    "    \n",
    "    def predict(self, model):\n",
    "        y = np.empty(shape=(len(self.identifiers), 28))\n",
    "        for n in range(len(self.identifiers)):\n",
    "            image = load_image(self.basepath, self.identifiers[n])\n",
    "            image = self.features_aumentation(self, image)\n",
    "            image = image.reshape((1, *image.shape))\n",
    "            #print(image.shape)\n",
    "            y[n] = model.predict(image)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitter = RepeatedKFold(n_splits=3, n_repeats=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partitions = []\n",
    "\n",
    "for train_idx, test_idx in splitter.split(train_labels.index.values):\n",
    "    partition = {}\n",
    "    partition[\"train\"] = train_labels.Id.values[train_idx]\n",
    "    partition[\"validation\"] = train_labels.Id.values[test_idx]\n",
    "    partitions.append(partition)\n",
    "    #print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    #print(\"TRAIN:\", len(train_idx), \"TEST:\", len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10357"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(partition[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define generator of data with preprocessing: validation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(list_IDs= partition['train'],\n",
    "                                   batch_size = 64,\n",
    "                                   shuffle = True,\n",
    "                                  labels_path= labels_path, \n",
    "                                   basepath= train_path, \n",
    "                                  grad = True, \n",
    "                                   dir_grad = True, \n",
    "                                   grad_threshold = True,\n",
    "                                 nb_rot= 4, \n",
    "                                   nb_threshold=6, \n",
    "                                   reduce=True, \n",
    "                                   block_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_generator = DataGenerator(partition['validation'],\n",
    "                                   batch_size = 64,\n",
    "                                   shuffle = True,\n",
    "                                  labels_path= labels_path, \n",
    "                                   basepath= train_path, \n",
    "                                  grad = True, \n",
    "                                   dir_grad = True, \n",
    "                                   grad_threshold = True,\n",
    "                                 nb_rot= 4, \n",
    "                                   nb_threshold=6, \n",
    "                                   reduce=True, \n",
    "                                   block_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_generator = PredictGenerator(partition['validation'], train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_crop_shape(target, refer):\n",
    "    # width, the 3rd dimension\n",
    "    cw = (target.get_shape()[2] - refer.get_shape()[2]).value\n",
    "    assert (cw >= 0)\n",
    "    if cw % 2 != 0:\n",
    "        cw1, cw2 = int(cw/2), int(cw/2) + 1\n",
    "    else:\n",
    "        cw1, cw2 = int(cw/2), int(cw/2)\n",
    "    # height, the 2nd dimension\n",
    "    ch = (target.get_shape()[1] - refer.get_shape()[1]).value\n",
    "    assert (ch >= 0)\n",
    "    if ch % 2 != 0:\n",
    "        ch1, ch2 = int(ch/2), int(ch/2) + 1\n",
    "    else:\n",
    "        ch1, ch2 = int(ch/2), int(ch/2)\n",
    "\n",
    "    return (ch1, ch2), (cw1, cw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseLineModel:\n",
    "    \n",
    "    def __init__(self, n_epochs):\n",
    "\n",
    "        self.input_shape = train_generator.shape_features\n",
    "        self.my_metrics = ['accuracy', fbeta_score_macro]\n",
    "        self.n_epochs = n_epochs\n",
    "        #print(self.input_shape)\n",
    "    \n",
    "    def build_model(self):\n",
    "        \n",
    "        #Useless model\n",
    "        '''self.model = Sequential()'''\n",
    "        '''self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu',input_shape=self.input_shape))\n",
    "        self.model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Dropout(0.25))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(64, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(28, activation='sigmoid'))'''\n",
    "        \n",
    "        #Useless model 2\n",
    "        input = Input(shape = self.input_shape )\n",
    "        \n",
    "        x= input\n",
    "        \n",
    "        A = Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        A = MaxPooling2D(pool_size=2, strides=1, padding=\"valid\")(A)\n",
    "        A= BatchNormalization()(A)\n",
    "\n",
    "        B = Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu', kernel_regularizer=l2(0.001))(A)\n",
    "        B = MaxPooling2D(pool_size=2, strides=1, padding=\"valid\")(B)\n",
    "        B= BatchNormalization()(B)\n",
    "\n",
    "        C = Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu', kernel_regularizer=l2(0.001))(B)\n",
    "        C = MaxPooling2D(pool_size=2, strides=1, padding=\"valid\")(C)\n",
    "        C= BatchNormalization()(C)\n",
    "\n",
    "        a = Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        a = MaxPooling2D(pool_size=2, strides=1, padding=\"valid\")(a)\n",
    "        a= BatchNormalization()(a)\n",
    "\n",
    "        b = Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        b = MaxPooling2D(pool_size=2, strides=1, padding=\"valid\")(b)\n",
    "        b= BatchNormalization()(b)\n",
    "\n",
    "        c = Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        c = MaxPooling2D(pool_size=2, strides=1, padding=\"valid\")(c)\n",
    "        c= BatchNormalization()(c)\n",
    "\n",
    "        x = add([a,b,c])\n",
    "\n",
    "        C= Flatten()(C)\n",
    "        C=Dropout(0.3)(C)\n",
    "\n",
    "        x= Flatten()(x)\n",
    "        x=Dropout(0.3)(x)\n",
    "\n",
    "        x = Dense((256),activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        C = Dense((256),activation='relu', kernel_regularizer=l2(0.001))(C)\n",
    "\n",
    "        x = add([x,C])\n",
    "\n",
    "        x=Dropout (0.3)(x)\n",
    "        x=Dense((512),activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "\n",
    "        output = Dense(28, activation = \"sigmoid\")(x)\n",
    "\n",
    "        self.model = Model(inputs=[input], outputs=[output])\n",
    "        \n",
    "        #Spoon model\n",
    "        '''concat_axis = 3\n",
    "        input = Input(self.input_shape)\n",
    "\n",
    "        conv1 = Conv2D(32, (3, 3), padding=\"same\", name=\"conv1_1\", kernel_initializer='uniform',activation=\"relu\", data_format=\"channels_last\")(input)\n",
    "        conv1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv1)\n",
    "        pool1= BatchNormalization()(pool1)\n",
    "\n",
    "        conv2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(pool1)\n",
    "        conv2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv2)\n",
    "        pool2= BatchNormalization()(pool2)\n",
    "\n",
    "        conv3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(pool2)\n",
    "        conv3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv3)\n",
    "        pool3= BatchNormalization()(pool3)\n",
    "\n",
    "        conv4 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(pool3)\n",
    "        conv4 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(conv4)\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv4)\n",
    "        pool4= BatchNormalization()(pool4)\n",
    "\n",
    "        conv5 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(pool4)\n",
    "        conv5 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(conv5)\n",
    "\n",
    "        up_conv5 = UpSampling2D(size=(2, 2),data_format=\"channels_last\")(conv5)\n",
    "        ch, cw = get_crop_shape(conv4, up_conv5)\n",
    "        crop_conv4 = Cropping2D(cropping=(ch,cw), data_format=\"channels_last\")(conv4)\n",
    "        up6   = concatenate([up_conv5, crop_conv4], axis=concat_axis)\n",
    "        conv6 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(up6)\n",
    "        conv6 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(conv6)\n",
    "\n",
    "        up_conv6 = UpSampling2D(size=(2, 2),data_format=\"channels_last\")(conv6)\n",
    "        ch, cw = get_crop_shape(conv3, up_conv6)\n",
    "        crop_conv3 = Cropping2D(cropping=(ch,cw), data_format=\"channels_last\")(conv3)\n",
    "        up7   = concatenate([up_conv6, crop_conv3], axis=concat_axis)\n",
    "        conv7 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(up7)\n",
    "        conv7 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(conv7)\n",
    "\n",
    "        up_conv7 = UpSampling2D(size=(2, 2), data_format=\"channels_last\")(conv7)\n",
    "        ch, cw = get_crop_shape(conv2, up_conv7)\n",
    "        crop_conv2 = Cropping2D(cropping=(ch,cw), data_format=\"channels_last\")(conv2)\n",
    "        up8   = concatenate([up_conv7, crop_conv2], axis=concat_axis)\n",
    "        conv8 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(up8)\n",
    "        conv8 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform',data_format=\"channels_last\")(conv8)\n",
    "\n",
    "        up_conv8 = UpSampling2D(size=(2, 2), data_format=\"channels_last\")(conv8)\n",
    "        ch, cw = get_crop_shape(conv1, up_conv8)\n",
    "        crop_conv1 = Cropping2D(cropping=(ch,cw), data_format=\"channels_last\")(conv1)\n",
    "        up9   = concatenate([up_conv8, crop_conv1], axis=concat_axis)\n",
    "        conv9 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform', data_format=\"channels_last\")(up9)\n",
    "        conv9 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer='uniform', data_format=\"channels_last\")(conv9)\n",
    "\n",
    "        x= conv9\n",
    "\n",
    "        A = Conv2D(32, kernel_size=3, strides=2, padding='valid', activation='relu', kernel_initializer='uniform',kernel_regularizer=l2(0.001))(x)\n",
    "        A = MaxPooling2D(pool_size=2, strides=1, padding=\"valid\")(A)\n",
    "        A= BatchNormalization()(A)\n",
    "\n",
    "        B = Conv2D(32, kernel_size=3, strides=2, padding='valid', activation='relu', kernel_initializer='uniform',kernel_regularizer=l2(0.001))(A)\n",
    "        B = MaxPooling2D(pool_size=2, strides=1, padding=\"valid\")(B)\n",
    "        B= BatchNormalization()(B)\n",
    "\n",
    "        C = Conv2D(32, kernel_size=3, strides=1, padding='valid', activation='relu', kernel_initializer='uniform',kernel_regularizer=l2(0.001))(B)\n",
    "        C = MaxPooling2D(pool_size=2, strides=1, padding=\"valid\")(C)\n",
    "        C= BatchNormalization()(C)\n",
    "\n",
    "        D = Conv2D(32, kernel_size=3, strides=1, padding='valid', activation='relu', kernel_initializer='uniform',kernel_regularizer=l2(0.001))(C)\n",
    "        D = MaxPooling2D(pool_size=2, strides=1, padding=\"valid\")(D)\n",
    "        D = BatchNormalization()(D)\n",
    "\n",
    "        E = Conv2D(32, kernel_size=3, strides=1, padding='valid', activation='relu', kernel_initializer='uniform',kernel_regularizer=l2(0.001))(D)\n",
    "        E = MaxPooling2D(pool_size=2, strides=1, padding=\"valid\")(E)\n",
    "        E = BatchNormalization()(E)\n",
    "\n",
    "        F = Conv2D(32, kernel_size=3, strides=1, padding='valid', activation='relu', kernel_initializer='uniform',kernel_regularizer=l2(0.001))(E)\n",
    "        F = MaxPooling2D(pool_size=2, strides=1, padding=\"valid\")(F)\n",
    "        F = BatchNormalization()(F)\n",
    "\n",
    "        G = Conv2D(32, kernel_size=3, strides=1, padding='valid', activation='relu', kernel_initializer='uniform',kernel_regularizer=l2(0.001))(F)\n",
    "        G = MaxPooling2D(pool_size=2, strides=1, padding=\"valid\")(G)\n",
    "        G = BatchNormalization()(G)\n",
    "\n",
    "        H = Conv2D(32, kernel_size=3, strides=1, padding='valid', activation='relu', kernel_initializer='uniform',kernel_regularizer=l2(0.001))(G)\n",
    "        H = MaxPooling2D(pool_size=2, strides=1, padding=\"valid\")(H)\n",
    "        H = BatchNormalization()(H)\n",
    "\n",
    "        H= Flatten()(H)\n",
    "        H= Dropout(0.3)(H)\n",
    "\n",
    "        x = Dense((128),activation='relu', kernel_regularizer=l2(0.001), kernel_initializer='uniform')(H)\n",
    "\n",
    "        x=Dropout (0.3)(x)\n",
    "        x=Dense((256),activation='relu', kernel_regularizer=l2(0.001), kernel_initializer='uniform')(x)\n",
    "\n",
    "        output = Dense(28, activation = \"sigmoid\", kernel_initializer='uniform')(x)\n",
    "\n",
    "        self.model = Model(inputs=[input], outputs=[output])'''\n",
    "\n",
    "        return self.model\n",
    "        \n",
    "    def compile_model(self):\n",
    "        self.model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Nadam(),\n",
    "              metrics=self.my_metrics)\n",
    "    \n",
    "    def set_generators(self, train_generator, validation_generator):\n",
    "        self.training_generator = train_generator\n",
    "        self.validation_generator = validation_generator\n",
    "    \n",
    "    def learn(self):\n",
    "        return self.model.fit_generator(generator=self.training_generator,\n",
    "                    validation_data=self.validation_generator,\n",
    "                    epochs=self.n_epochs, \n",
    "                    use_multiprocessing=True,\n",
    "                    workers=8)\n",
    "    \n",
    "    def score(self):\n",
    "        return self.model.evaluate_generator(generator=self.validation_generator,\n",
    "                                      use_multiprocessing=True, \n",
    "                                      workers=8)\n",
    "    \n",
    "    def predict(self, predict_generator):\n",
    "        y = predict_generator.predict(self.model)\n",
    "        return y\n",
    "    \n",
    "    def save(self, modeloutputpath):\n",
    "        self.model.save(modeloutputpath)\n",
    "    \n",
    "    def load(self, modelinputpath):\n",
    "        self.model = load_model(modelinputpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaseLineModel for 10 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseLineModel(10)\n",
    "model.build_model()\n",
    "model.compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseLineModel' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-437440fcd987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Create graph nodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseLineModel' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model.set_generators(train_generator, validation_generator)\n",
    "history = model.learn()\n",
    "model.save(\"baseline_model.hdf5\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/yihui-he/u-net/blob/master/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def Unet():\n",
    "    inputs = Input(self.input_shape)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(pool1)\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(pool2)\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(pool3)\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(pool4)\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(conv5)\n",
    "    \n",
    "    up6 = merge([Convolution2D(256, 2, 2,activation='relu', border_mode='same')(UpSampling2D(size=(2, 2))(conv5)), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(up6)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv6)\n",
    "\n",
    "    up7 = merge([Convolution2D(128, 2, 2,activation='relu', border_mode='same')(UpSampling2D(size=(2, 2))(conv6)), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(up7)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv7)\n",
    "\n",
    "    up8 = merge([Convolution2D(64, 2, 2,activation='relu', border_mode='same')(UpSampling2D(size=(2, 2))(conv7)), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(up8)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv8)\n",
    "\n",
    "    up9 = merge([Convolution2D(32, 2, 2,activation='relu', border_mode='same')(UpSampling2D(size=(2, 2))(conv8)), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(up9)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv9)\n",
    "\n",
    "    conv10 = Convolution2D(1, 1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    return model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
