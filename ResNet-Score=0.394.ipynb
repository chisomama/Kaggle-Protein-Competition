{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from old.fastai.conv_learner import *\n",
    "from old.fastai.dataset import * \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This current baseline is a bunch of Resnets in series. And that's where Chisom is staying.\n",
    "#Maybe I could explore GRUs.\n",
    "\n",
    "PATH = './'\n",
    "TRAIN = 'train/'\n",
    "TEST = 'test/'\n",
    "LABELS = './train.csv'\n",
    "SAMPLE = './sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_label_dict = {\n",
    "0:  'Nucleoplasm',\n",
    "1:  'Nuclear membrane',\n",
    "2:  'Nucleoli',   \n",
    "3:  'Nucleoli fibrillar center',\n",
    "4:  'Nuclear speckles',\n",
    "5:  'Nuclear bodies',\n",
    "6:  'Endoplasmic reticulum',   \n",
    "7:  'Golgi apparatus',\n",
    "8:  'Peroxisomes',\n",
    "9:  'Endosomes',\n",
    "10:  'Lysosomes',\n",
    "11:  'Intermediate filaments',\n",
    "12:  'Actin filaments',\n",
    "13:  'Focal adhesion sites',   \n",
    "14:  'Microtubules',\n",
    "15:  'Microtubule ends',  \n",
    "16:  'Cytokinetic bridge',   \n",
    "17:  'Mitotic spindle',\n",
    "18:  'Microtubule organizing center',  \n",
    "19:  'Centrosome',\n",
    "20:  'Lipid droplets',\n",
    "21:  'Plasma membrane',   \n",
    "22:  'Cell junctions', \n",
    "23:  'Mitochondria',\n",
    "24:  'Aggresome',\n",
    "25:  'Cytosol',\n",
    "26:  'Cytoplasmic bodies',   \n",
    "27:  'Rods & rings' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = 2   #number of workers for data loader\n",
    "arch = resnet34 #specify target architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = list({f[:36] for f in os.listdir(TRAIN)})\n",
    "test_names = list({f[:36] for f in os.listdir(TEST)})\n",
    "tr_n, val_n = train_test_split(train_names, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_rgby(path,id): #a function that reads RGBY image\n",
    "    colors = ['red','green','blue','yellow']\n",
    "    flags = cv2.IMREAD_GRAYSCALE\n",
    "    img = [cv2.imread(os.path.join(path, id+'_'+color+'.png'), flags).astype(np.float32)/255\n",
    "           for color in colors]\n",
    "    return np.stack(img, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pdFilesDataset(FilesDataset):\n",
    "    def __init__(self, fnames, path, transform):\n",
    "        self.labels = pd.read_csv(LABELS).set_index('Id')\n",
    "        self.labels['Target'] = [[int(i) for i in s.split()] for s in self.labels['Target']]\n",
    "        super().__init__(fnames, transform, path)\n",
    "    \n",
    "    def get_x(self, i):\n",
    "        img = open_rgby(self.path,self.fnames[i])\n",
    "        if self.sz == 512: return img \n",
    "        else: return cv2.resize(img, (self.sz, self.sz),cv2.INTER_AREA)\n",
    "    \n",
    "    def get_y(self, i):\n",
    "        if(self.path == TEST): return np.zeros(len(name_label_dict),dtype=np.int)\n",
    "        else:\n",
    "            labels = self.labels.loc[self.fnames[i]]['Target']\n",
    "            return np.eye(len(name_label_dict),dtype=np.float)[labels].sum(axis=0)\n",
    "        \n",
    "    @property\n",
    "    def is_multi(self): return True\n",
    "    @property\n",
    "    def is_reg(self):return True\n",
    "    #this flag is set to remove the output sigmoid that allows log(sigmoid) optimization\n",
    "    #of the numerical stability of the loss function\n",
    "    \n",
    "    def get_c(self): return len(name_label_dict) #number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sz,bs):\n",
    "    #data augmentation\n",
    "    aug_tfms = [RandomRotate(30, tfm_y=TfmType.NO),\n",
    "                RandomDihedral(tfm_y=TfmType.NO),\n",
    "                RandomLighting(0.05, 0.05, tfm_y=TfmType.NO)]\n",
    "    #mean and std in of each channel in the train set\n",
    "    stats = A([0.08069, 0.05258, 0.05487, 0.08282], [0.13704, 0.10145, 0.15313, 0.13814])\n",
    "    tfms = tfms_from_stats(stats, sz, crop_type=CropType.NO, tfm_y=TfmType.NO, \n",
    "                aug_tfms=aug_tfms)\n",
    "    ds = ImageData.get_ds(pdFilesDataset, (tr_n[:-(len(tr_n)%bs)],TRAIN), \n",
    "                (val_n,TRAIN), tfms, test=(test_names,TEST))\n",
    "    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 4, 256, 256]), torch.Size([16, 28]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 16\n",
    "sz = 256\n",
    "md = get_data(sz,bs)\n",
    "\n",
    "x,y = next(iter(md.trn_dl))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(preds,targs,th=0.0):\n",
    "    preds = (preds > th).int()\n",
    "    targs = targs.int()\n",
    "    return (preds==targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvnetBuilder_custom():\n",
    "    def __init__(self, f, c, is_multi, is_reg, ps=None, xtra_fc=None, xtra_cut=0, \n",
    "                 custom_head=None, pretrained=True):\n",
    "        self.f,self.c,self.is_multi,self.is_reg,self.xtra_cut = f,c,is_multi,is_reg,xtra_cut\n",
    "        if xtra_fc is None: xtra_fc = [512]\n",
    "        if ps is None: ps = [0.25]*len(xtra_fc) + [0.5]\n",
    "        self.ps,self.xtra_fc = ps,xtra_fc\n",
    "\n",
    "        if f in model_meta: cut,self.lr_cut = model_meta[f]\n",
    "        else: cut,self.lr_cut = 0,0\n",
    "        cut-=xtra_cut\n",
    "        layers = cut_model(f(pretrained), cut)\n",
    "        \n",
    "        #replace first convolutional layer by 4->64 while keeping corresponding weights\n",
    "        #and initializing new weights with zeros\n",
    "        w = layers[0].weight\n",
    "        layers[0] = nn.Conv2d(4,64,kernel_size=(7,7),stride=(2,2),padding=(3, 3), bias=False)\n",
    "        layers[0].weight = torch.nn.Parameter(torch.cat((w,torch.zeros(64,1,7,7)),dim=1))\n",
    "        \n",
    "        self.nf = model_features[f] if f in model_features else (num_features(layers)*2)\n",
    "        if not custom_head: layers += [AdaptiveConcatPool2d(), Flatten()]\n",
    "        self.top_model = nn.Sequential(*layers)\n",
    "\n",
    "        n_fc = len(self.xtra_fc)+1\n",
    "        if not isinstance(self.ps, list): self.ps = [self.ps]*n_fc\n",
    "\n",
    "        if custom_head: fc_layers = [custom_head]\n",
    "        else: fc_layers = self.get_fc_layers()\n",
    "        self.n_fc = len(fc_layers)\n",
    "        self.fc_model = to_gpu(nn.Sequential(*fc_layers))\n",
    "        if not custom_head: apply_init(self.fc_model, kaiming_normal)\n",
    "        self.model = to_gpu(nn.Sequential(*(layers+fc_layers)))\n",
    "\n",
    "    @property\n",
    "    def name(self): return f'{self.f.__name__}_{self.xtra_cut}'\n",
    "\n",
    "    def create_fc_layer(self, ni, nf, p, actn=None):\n",
    "        res=[nn.BatchNorm1d(num_features=ni)]\n",
    "        if p: res.append(nn.Dropout(p=p))\n",
    "        res.append(nn.Linear(in_features=ni, out_features=nf))\n",
    "        if actn: res.append(actn)\n",
    "        return res\n",
    "\n",
    "    def get_fc_layers(self):\n",
    "        res=[]\n",
    "        ni=self.nf\n",
    "        for i,nf in enumerate(self.xtra_fc):\n",
    "            res += self.create_fc_layer(ni, nf, p=self.ps[i], actn=nn.ReLU())\n",
    "            ni=nf\n",
    "        final_actn = nn.Sigmoid() if self.is_multi else nn.LogSoftmax()\n",
    "        if self.is_reg: final_actn = None\n",
    "        res += self.create_fc_layer(ni, self.c, p=self.ps[-1], actn=final_actn)\n",
    "        return res\n",
    "\n",
    "    def get_layer_groups(self, do_fc=False):\n",
    "        if do_fc:\n",
    "            return [self.fc_model]\n",
    "        idxs = [self.lr_cut]\n",
    "        c = children(self.top_model)\n",
    "        if len(c)==3: c = children(c[0])+c[1:]\n",
    "        lgs = list(split_by_idxs(c,idxs))\n",
    "        return lgs+[self.fc_model]\n",
    "    \n",
    "class ConvLearner(Learner):\n",
    "    def __init__(self, data, models, precompute=False, **kwargs):\n",
    "        self.precompute = False\n",
    "        super().__init__(data, models, **kwargs)\n",
    "        if hasattr(data, 'is_multi') and not data.is_reg and self.metrics is None:\n",
    "            self.metrics = [accuracy_thresh(0.5)] if self.data.is_multi else [accuracy]\n",
    "        if precompute: self.save_fc1()\n",
    "        self.freeze()\n",
    "        self.precompute = precompute\n",
    "\n",
    "    def _get_crit(self, data):\n",
    "        if not hasattr(data, 'is_multi'): return super()._get_crit(data)\n",
    "\n",
    "        return F.l1_loss if data.is_reg else F.binary_cross_entropy if data.is_multi else F.nll_loss\n",
    "\n",
    "    @classmethod\n",
    "    def pretrained(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n",
    "                   pretrained=True, **kwargs):\n",
    "        models = ConvnetBuilder_custom(f, data.c, data.is_multi, data.is_reg,\n",
    "            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=pretrained)\n",
    "        return cls(data, models, precompute, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def lsuv_learner(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n",
    "                  needed_std=1.0, std_tol=0.1, max_attempts=10, do_orthonorm=False, **kwargs):\n",
    "        models = ConvnetBuilder(f, data.c, data.is_multi, data.is_reg,\n",
    "            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=False)\n",
    "        convlearn=cls(data, models, precompute, **kwargs)\n",
    "        convlearn.lsuv_init()\n",
    "        return convlearn\n",
    "    \n",
    "    @property\n",
    "    def model(self): return self.models.fc_model if self.precompute else self.models.model\n",
    "    \n",
    "    def half(self):\n",
    "        if self.fp16: return\n",
    "        self.fp16 = True\n",
    "        if type(self.model) != FP16: self.models.model = FP16(self.model)\n",
    "        if not isinstance(self.models.fc_model, FP16): self.models.fc_model = FP16(self.models.fc_model)\n",
    "    def float(self):\n",
    "        if not self.fp16: return\n",
    "        self.fp16 = False\n",
    "        if type(self.models.model) == FP16: self.models.model = self.model.module.float()\n",
    "        if type(self.models.fc_model) == FP16: self.models.fc_model = self.models.fc_model.module.float()\n",
    "\n",
    "    @property\n",
    "    def data(self): return self.fc_data if self.precompute else self.data_\n",
    "\n",
    "    def create_empty_bcolz(self, n, name):\n",
    "        return bcolz.carray(np.zeros((0,n), np.float32), chunklen=1, mode='w', rootdir=name)\n",
    "\n",
    "    def set_data(self, data, precompute=False):\n",
    "        super().set_data(data)\n",
    "        if precompute:\n",
    "            self.unfreeze()\n",
    "            self.save_fc1()\n",
    "            self.freeze()\n",
    "            self.precompute = True\n",
    "        else:\n",
    "            self.freeze()\n",
    "\n",
    "    def get_layer_groups(self):\n",
    "        return self.models.get_layer_groups(self.precompute)\n",
    "\n",
    "    def summary(self):\n",
    "        precompute = self.precompute\n",
    "        self.precompute = False\n",
    "        res = super().summary()\n",
    "        self.precompute = precompute\n",
    "        return res\n",
    "\n",
    "    def get_activations(self, force=False):\n",
    "        tmpl = f'_{self.models.name}_{self.data.sz}.bc'\n",
    "        # TODO: Somehow check that directory names haven't changed (e.g. added test set)\n",
    "        names = [os.path.join(self.tmp_path, p+tmpl) for p in ('x_act', 'x_act_val', 'x_act_test')]\n",
    "        if os.path.exists(names[0]) and not force:\n",
    "            self.activations = [bcolz.open(p) for p in names]\n",
    "        else:\n",
    "            self.activations = [self.create_empty_bcolz(self.models.nf,n) for n in names]\n",
    "\n",
    "    def save_fc1(self):\n",
    "        self.get_activations()\n",
    "        act, val_act, test_act = self.activations\n",
    "        m=self.models.top_model\n",
    "        if len(self.activations[0])!=len(self.data.trn_ds):\n",
    "            predict_to_bcolz(m, self.data.fix_dl, act)\n",
    "        if len(self.activations[1])!=len(self.data.val_ds):\n",
    "            predict_to_bcolz(m, self.data.val_dl, val_act)\n",
    "        if self.data.test_dl and (len(self.activations[2])!=len(self.data.test_ds)):\n",
    "            if self.data.test_dl: predict_to_bcolz(m, self.data.test_dl, test_act)\n",
    "\n",
    "        self.fc_data = ImageClassifierData.from_arrays(self.data.path,\n",
    "                (act, self.data.trn_y), (val_act, self.data.val_y), self.data.bs, classes=self.data.classes,\n",
    "                test = test_act if self.data.test_dl else None, num_workers=8)\n",
    "\n",
    "    def freeze(self):\n",
    "        self.freeze_to(-1)\n",
    "\n",
    "    def unfreeze(self):\n",
    "        self.freeze_to(0)\n",
    "        self.precompute = False\n",
    "\n",
    "    def predict_array(self, arr):\n",
    "        precompute = self.precompute\n",
    "        self.precompute = False\n",
    "        pred = super().predict_array(arr)\n",
    "        self.precompute = precompute\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 256 #image size\n",
    "bs = 64  #batch size\n",
    "\n",
    "md = get_data(sz,bs)\n",
    "learner = ConvLearner.pretrained(arch, md, ps=0.5) #dropout 50%\n",
    "learner.opt_fn = optim.Adam\n",
    "learner.clip = 1.0 #gradient clipping\n",
    "learner.crit = FocalLoss()\n",
    "learner.metrics = [acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ConvLearner.summary of Sequential(\n",
       "  (0): Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveConcatPool2d(\n",
       "    (ap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (mp): AdaptiveMaxPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (9): Flatten()\n",
       "  (10): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): Dropout(p=0.5)\n",
       "  (12): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (13): ReLU()\n",
       "  (14): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (15): Dropout(p=0.5)\n",
       "  (16): Linear(in_features=512, out_features=28, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-2\n",
    "learner.unfreeze()\n",
    "lrs=np.array([lr/10,lr/3,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98466ec460d4137a814769b0c63a212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=25, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   acc                        \n",
      "    0      1.302977   1.183923   0.951174  \n",
      "    1      1.107111   1.048477   0.950117                   \n",
      "    2      0.984476   0.874637   0.959494                    \n",
      "    3      0.914994   0.852536   0.959586                    \n",
      "    4      0.840847   0.896401   0.957897                    \n",
      "    5      0.801522   0.830347   0.962838                    \n",
      "    6      0.776531   0.773436   0.963596                    \n",
      "    7      0.745426   0.763416   0.966159                    \n",
      " 78%|███████▊  | 342/436 [14:49<04:27,  2.85s/it, loss=0.715]"
     ]
    }
   ],
   "source": [
    "lr = 2e-2\n",
    "lrs=np.array([lr/10,lr/3,lr])\n",
    "learner.fit(lrs/16,1,cycle_len=25,use_clr=(5,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('ResNet34_256_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_np(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "preds,y = learner.TTA(n_aug=16)\n",
    "preds = np.stack(preds, axis=-1)\n",
    "preds = sigmoid_np(preds)\n",
    "pred = preds.max(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_soft(preds,targs,th=0.5,d=50.0):\n",
    "    preds = sigmoid_np(d*(preds - th))\n",
    "    targs = targs.astype(np.float)\n",
    "    score = 2.0*(preds*targs).sum(axis=0)/((preds+targs).sum(axis=0) + 1e-6)\n",
    "    return score\n",
    "\n",
    "def fit_val(x,y):\n",
    "    params = 0.5*np.ones(len(name_label_dict))\n",
    "    wd = 1e-5\n",
    "    error = lambda p: np.concatenate((F1_soft(x,y,p) - 1.0,\n",
    "                                      wd*(p - 0.5)), axis=None)\n",
    "    p, success = opt.leastsq(error, params)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = fit_val(pred,y)\n",
    "th[th<0.1] = 0.1\n",
    "print('Thresholds: ',th)\n",
    "print('F1 macro: ',f1_score(y, pred>th, average='macro'))\n",
    "print('F1 macro (th = 0.5): ',f1_score(y, pred>0.5, average='macro'))\n",
    "print('F1 micro: ',f1_score(y, pred>th, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fractions: ',(pred > th).mean(axis=0))\n",
    "print('Fractions (true): ',(y > th).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_t,y_t = learner.TTA(n_aug=16,is_test=True)\n",
    "preds_t = np.stack(preds_t, axis=-1)\n",
    "preds_t = sigmoid_np(preds_t)\n",
    "pred_t = preds_t.max(axis=-1) #max works better for F1 macro score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pred(pred, th=0.5, fname='protein_classification.csv'):\n",
    "    pred_list = []\n",
    "    for line in pred:\n",
    "        s = ' '.join(list([str(i) for i in np.nonzero(line>th)[0]]))\n",
    "        pred_list.append(s)\n",
    "        \n",
    "    sample_df = pd.read_csv(SAMPLE)\n",
    "    sample_list = list(sample_df.Id)\n",
    "    pred_dic = dict((key, value) for (key, value) \n",
    "                in zip(learner.data.test_ds.fnames,pred_list))\n",
    "    pred_list_cor = [pred_dic[id] for id in sample_list]\n",
    "    df = pd.DataFrame({'Id':sample_list,'Predicted':pred_list_cor})\n",
    "    df.to_csv(fname, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_t = np.array([0.565,0.39,0.55,0.345,0.33,0.39,0.33,0.45,0.38,0.39,\n",
    "               0.34,0.42,0.31,0.38,0.49,0.50,0.38,0.43,0.46,0.40,\n",
    "               0.39,0.505,0.37,0.47,0.41,0.545,0.32,0.1])\n",
    "print('Fractions: ',(pred_t > th_t).mean(axis=0))\n",
    "save_pred(pred_t,th_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_prob = [\n",
    " 0.362397820,0.043841336,0.075268817,0.059322034,0.075268817,\n",
    " 0.075268817,0.043841336,0.075268817,0.010000000,0.010000000,\n",
    " 0.010000000,0.043841336,0.043841336,0.014198783,0.043841336,\n",
    " 0.010000000,0.028806584,0.014198783,0.028806584,0.059322034,\n",
    " 0.010000000,0.126126126,0.028806584,0.075268817,0.010000000,\n",
    " 0.222493880,0.028806584,0.010000000]\n",
    "# I replaced 0 by 0.01 since there may be a rounding error leading to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_soft(preds,th=0.5,d=50.0):\n",
    "    preds = sigmoid_np(d*(preds - th))\n",
    "    return preds.mean(axis=0)\n",
    "\n",
    "def fit_test(x,y):\n",
    "    params = 0.5*np.ones(len(name_label_dict))\n",
    "    wd = 1e-5\n",
    "    error = lambda p: np.concatenate((Count_soft(x,p) - y,\n",
    "                                      wd*(p - 0.5)), axis=None)\n",
    "    p, success = opt.leastsq(error, params)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_t = fit_test(pred_t,lb_prob)\n",
    "th_t[th_t<0.1] = 0.1\n",
    "print('Thresholds: ',th_t)\n",
    "print('Fractions: ',(pred_t > th_t).mean(axis=0))\n",
    "print('Fractions (th = 0.5): ',(pred_t > 0.5).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pred(pred_t,th_t,'protein_classification_f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pred(pred_t,th,'protein_classification_v.csv')\n",
    "save_pred(pred_t,0.5,'protein_classification_05.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = [8,9,10,15,20,24,27]\n",
    "for i in class_list:\n",
    "    th_t[i] = th[i]\n",
    "save_pred(pred_t,th_t,'protein_classification_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(LABELS).set_index('Id')\n",
    "label_count = np.zeros(len(name_label_dict))\n",
    "for label in labels['Target']:\n",
    "    l = [int(i) for i in label.split()]\n",
    "    label_count += np.eye(len(name_label_dict))[l].sum(axis=0)\n",
    "label_fraction = label_count.astype(np.float)/len(labels)\n",
    "label_count, label_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_t = fit_test(pred_t,label_fraction)\n",
    "th_t[th_t<0.05] = 0.05\n",
    "print('Thresholds: ',th_t)\n",
    "print('Fractions: ',(pred_t > th_t).mean(axis=0))\n",
    "save_pred(pred_t,th_t,'protein_classification_t1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
